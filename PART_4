from random import random
import numpy as np

dataset = [[2.7810836, 4.550537003, 0], [3.396561688, 4.400293529, 0], [1.38807019, 1.850220317, 0],
           [3.06409232, 3.005305973, 0], [7.627531214, 2.759262235, 1],[5.332441248, 2.088626775, 1], [6.922596716, 1.77106367, 1]]

test_data = [[1.465489372, 2.362125076, 0], [8.675418651, -0.242068655, 1], [7.673756466, 3.508563011, 1]] #(x의 좌표, y의 좌표, 0 or 1)

n_inputs = 2 #x 와 y의 정보
n_outputs = 2 #0 or 1의 정보

#신경망 구현
def initialize_network(n_inputs, n_hidden, n_outputs):
    network = list()
    hidden_layer1 = [{'weights' : [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)] # +1은 bias이다.
    hidden_layer2 = [{'weights' : [random() for i in range(n_hidden + 1)]} for i in range(n_hidden)] #두 개의 레이어 생성
    output_layer = [{'weights' : [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)] #output 레이어 생성
    
    network.append(hidden_layer1)
    network.append(hidden_layer2)
    network.append(output_layer) #list에 추가
    return network

#순전파 구현
def net_input(weights, inputs):
    total_input = weights[-1]
    for i in range(len(weights)-1):
        total_input += weights[i] * inputs[i]
    return total_input

def activation(total_input):
    return 1.0 / (1.0 + np.exp(-total_input)) #sigmoid 형태 식을 풀어 쓴 것이다.

def forward_propagate(network, row):
    inputs = row
    for layer in network:
        outputs = []
        for neuron in layer:
            total_input = net_input(neuron['weights'], inputs)
            neuron['output'] = activation(total_input)
            outputs.append(neuron['output'])
        inputs = outputs
    return inputs

#역전파 구현
def cost_function(expected, outputs):
    n = len(expected)
    total_error = 0.0
    
    for i in range(n):
        total_error += (expected[i] - outputs[i]) ** 2
    return total_error #error function임

def transfer_derivative(output):
    return output * (1.0 - output) #sigmoid를 output에 대하여 미분한 꼴이다.

def backward_propagate(network, expected): #(업데이트 해야 하는 네트워크 모델 <list>, 실제 값)
    for i in reversed(range(len(network))):
        layer = network[i] #layer를 구한다.
        errors = list() #예측값 오차를 저장할 list, 여기서 오차는 층마다의 반복 누적이다.
        
        if i == len(network) - 1: #마지막 층(출력층) 처리
            for j in range(len(layer)):
                neuron = layer[j]
                error = -2 * (expected[j] - neuron['output'])
                errors.append(error)
        else: #모든 출력 오차를 더해서 저장
            for j in range(len(layer)):
                error = 0.0
                for neuron in network[i + 1]:
                    error += (neuron['weights'][j] * neuron['delta'])
                errors.append(error)
        for j in range(len(layer)): #sigmoid 미분값 구한다. -> 가중치 업데이트 다음 코드에서 시작.
            neuron = layer[j]
            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])

def update_weights(network, row, learning_rate):
    for i in range(len(network)):
        inputs = row[:-1] 
        if i != 0:
            inputs = [neuron['output'] for neuron in network[i - 1]] #초기 데이터로 초기화
        for neuron in network[i]:
            for j in range(len(inputs)):
                neuron['weights'][j] -= learning_rate * neuron['delta'] * inputs[j] #가중치 업데이트 -> 변수 조정
            neuron['weights'][-1] -= learning_rate * neuron['delta'] #bias 처리

def train_network(network, training_data, learning_rate, n_epoch, n_outputs): #위 함수들을 사용하여 네트워크를 훈련시키는 함수 구현
    for epoch in range(n_epoch): #훈련 횟수 설정
        sum_error = 0 #loss
        for row in training_data:
            outputs = forward_propagate(network, row) #순전파 시행, 0과 1 중 확률이 더 높은 수를 반환할 것
            expected = [0 for i in range(n_outputs)] #기대 출력을 0으로 설정
            expected[row[-1]] = 1 #기대 출력 중 대응하는 위치를 1로 설정
            sum_error += cost_function(expected, outputs) #오차 계산
            backward_propagate(network, expected)
            update_weights(network, row, learning_rate) #delta 속성 설정, 가중치 조절
        print('>epoch : %d, learning rate : %.3f, error : %.3f' % (epoch, learning_rate, sum_error))

def predict(network, row):
    outputs = forward_propagate(network, row)
    return outputs.index(max(outputs)) #가장 큰 확률을 가진 클래스 선택

#실행
network = initialize_network(n_inputs, 1, n_outputs) #hidden layer 1개
train_network(network, training_data = dataset, learning_rate = 0.5, n_epoch = 2000, n_outputs = n_outputs) #하이퍼파라미터 임의 설정

for row in test_data:
    result = predict(network, row)
    print('expected : %d, predicted : %d\n' % (row[-1], result)) #end, network 가중치의 초기값은 랜덤이다.
